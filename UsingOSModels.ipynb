{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0a0982",
   "metadata": {},
   "source": [
    "# Setup\n",
    "For this project we need the torch, transformers and sentencepiece Python packages in order to load and use pre-trained models. We will also need the huggingface_hub package to programmatically login to the Hugging Face Hub and manage repositories, and the datasets package to download datasets from the Hub.\n",
    "\n",
    "The sentencepiece package is required by transformers to perform inference with some of the pre-trained open source models on Hugging Face Hub and does not need to be explicitly imported. Import the remaining packages as follows.\n",
    "\n",
    "Run the provided !pip code to install necessary packages and restart your kernel.\n",
    "Import torch\n",
    "Import huggingface_hub using the alias hf_hub.\n",
    "Import datasets\n",
    "Import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da8dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install huggingface_hub\n",
    "# %pip install pyarrow\n",
    "# %pip install transformers\n",
    "# %pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e008a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import os\n",
    "import torch # Import torch\n",
    "import huggingface_hub as hf_hub # Import huggingface_hub using the alias hf_hub\n",
    "import datasets # Import datasets \n",
    "import transformers # Import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38065f",
   "metadata": {},
   "source": [
    "# Downloading pre-trained models\n",
    "Hugging Face Hub as a Git Platform\n",
    "\n",
    "The Hugging Face website (also known as the Hub) is essentially a Git platform designed to store pretrained models and datasets as Git repositories. Similar to GitHub, it allows users to explore, create, clone, push repositories and so much more. Each pretrained checkpoint has its own repository and in most cases a descriptive README with code snippets to load and run the model. See the \n",
    "bert-base-cased\n",
    " model repository as an example.\n",
    "\n",
    "How to Use Pretrained Models\n",
    "\n",
    "While the Hub is a great place to explore different tasks and pretrained models, we need the transformers or diffusers libraries in order to load and make predictions with pre-trained models. These two libraries reimplement the code of the state-of-the-art ML research such that vastly different models can be downloaded, loaded into memory and used in a unified way with a few lines of code.\n",
    "\n",
    "In this task, you will learn how to use the Auto classes of transformers and the from_pretrained method to download and load any model on the Hugging Face Hub. For a full list of supported models, refer to the GitHub \n",
    "README\n",
    ".\n",
    "\n",
    "What is the Auto Class?\n",
    "\n",
    "Auto classes of the transformers are simply tools to load models and their data preprocessors in a unified way. Remember, the library reimplements each model such that they each have their own class (BertModel, RobertaModel, T5Model, etc.) with mostly uniform input and output data format across all models. transformers have the following Auto class types to load models and their data preprocessors:\n",
    "\n",
    "AutoModel\n",
    "AutoModelForTASK> (more on this below)\n",
    "AutoTokenizer\n",
    "AutoFeatureExtractor\n",
    "AutoImageProcessor\n",
    "AutoProcessor\n",
    "Loading Models into Memory with from_pretrained\n",
    "\n",
    "For the first task, you will download the pretrained \"cardiffnlp/twitter-roberta-base-emoji\" model and load the model and its data preprocessor into memory with the from_pretrained(<REPO_NAME_OR_PATH>) method. The \"cardiffnlp/twitter-roberta-base-emoji\" is a text classification model that is trained to predict the emoji class ID of a given tweet.\n",
    "\n",
    "cardiffnlp/twitter-roberta-base-emoji\n",
    " is a valid Git repository on the Hub and the from_pretrained() method downloads and uses the tokenizer specific \n",
    "files\n",
    " from the model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7bdb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the AutoTokenizer and AutoModel classes from transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained tokenizer of the \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ea5d2",
   "metadata": {},
   "source": [
    "What from_pretrained() does\n",
    "\n",
    "The from_pretrained() method first searches for a model repository with the same name on the Hugging Face Hub but it also accepts a local path or a URL with the expected folder structure. You can simply git clone the repository and load it from your local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3e89a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizerFast(name_or_path='cardiffnlp/twitter-roberta-base-emoji', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenizer to see the data preprocessing configuration for this model \n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df85c2b",
   "metadata": {},
   "source": [
    "Tokenizers\n",
    "\n",
    "NLP models can't process text inputs as it is and need to be converted to a fixed length mathematical format. The Tokenizer classes preprocess the text such that each word and punctuation is given a unique ID or a token, short sentences are padded and long sentences are truncated to create fixed-size input vectors. They also allow using additional tokens such as bos, eos, unk, sep, and more to specify start and end of sentences, and to assign token IDS to unknown words that are not in the tokenizer vocabulary.\n",
    "\n",
    "The output of the tokenizer tells us this pretrained model uses a tokenizer with a 50265 unique token IDs that applies padding or truncation to the end of each input text, and removes leading (leftside) extra whitespaces. You can always refer to the corresponding Tokenizer documentation to learn more about each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ebcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tiya\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-emoji. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emoji and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model of the \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d85bcb",
   "metadata": {},
   "source": [
    "Notes on Model Loading\n",
    "\n",
    "Take a look at the warning above. We were only able to load a chunk of the model parameters included in the checkpoint with the AutoModel class.\n",
    "\n",
    "This is because all transformers models are designed to have a single base class and multiple task-specific prediction classes built on top of it. The AutoModel class is designed to only load the base model parameters such as RobertaModel but not task-specific models such as RobertaForSequenceClassification.\n",
    "\n",
    "In order to identify the exact class name and task of your target checkpoint, you can simply refer to the model configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
