{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0a0982",
   "metadata": {},
   "source": [
    "# Setup\n",
    "For this project we need the torch, transformers and sentencepiece Python packages in order to load and use pre-trained models. We will also need the huggingface_hub package to programmatically login to the Hugging Face Hub and manage repositories, and the datasets package to download datasets from the Hub.\n",
    "\n",
    "The sentencepiece package is required by transformers to perform inference with some of the pre-trained open source models on Hugging Face Hub and does not need to be explicitly imported. Import the remaining packages as follows.\n",
    "\n",
    "Run the provided !pip code to install necessary packages and restart your kernel.\n",
    "Import torch\n",
    "Import huggingface_hub using the alias hf_hub.\n",
    "Import datasets\n",
    "Import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4da8dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install huggingface_hub\n",
    "# %pip install pyarrow\n",
    "# %pip install transformers\n",
    "# %pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e008a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import os\n",
    "import torch # Import torch\n",
    "import huggingface_hub as hf_hub # Import huggingface_hub using the alias hf_hub\n",
    "import datasets # Import datasets \n",
    "import transformers # Import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38065f",
   "metadata": {},
   "source": [
    "# Downloading pre-trained models\n",
    "- Hugging Face Hub is a Git-based platform hosting model and dataset repos (each with a README and examples, e.g., bert-base-cased).\n",
    "- Use transformers or diffusers to load and run pretrained models.\n",
    "- Auto classes unify loading across architectures:\n",
    "    - AutoModel, AutoModelFor<TASK>, AutoTokenizer, AutoFeatureExtractor, AutoImageProcessor, AutoProcessor.\n",
    "- from_pretrained(<repo_or_path>) downloads configs, weights, and preprocessors.\n",
    "- In this task, load the “cardiffnlp/twitter-roberta-base-emoji” tokenizer and model—a tweet emoji classifier. from_pretrained() fetches the necessary tokenizer files from that repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf7bdb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer and AutoModel classes from transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained tokenizer of the \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ea5d2",
   "metadata": {},
   "source": [
    "What from_pretrained() does\n",
    "\n",
    "The from_pretrained() method first searches for a model repository with the same name on the Hugging Face Hub but it also accepts a local path or a URL with the expected folder structure. You can simply git clone the repository and load it from your local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f3e89a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizerFast(name_or_path='cardiffnlp/twitter-roberta-base-emoji', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenizer to see the data preprocessing configuration for this model \n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df85c2b",
   "metadata": {},
   "source": [
    "Tokenizers\n",
    "\n",
    "NLP models can't process text inputs as it is and need to be converted to a fixed length mathematical format. The Tokenizer classes preprocess the text such that each word and punctuation is given a unique ID or a token, short sentences are padded and long sentences are truncated to create fixed-size input vectors. They also allow using additional tokens such as bos, eos, unk, sep, and more to specify start and end of sentences, and to assign token IDS to unknown words that are not in the tokenizer vocabulary.\n",
    "\n",
    "The output of the tokenizer tells us this pretrained model uses a tokenizer with a 50265 unique token IDs that applies padding or truncation to the end of each input text, and removes leading (leftside) extra whitespaces. The tokenizer also returns attention masks that tell the model which tokens are actual input tokens and which ones are just padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25ebcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emoji and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model of the \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d85bcb",
   "metadata": {},
   "source": [
    "Notes on Model Loading\n",
    "\n",
    "Take a look at the warning above. We were only able to load a chunk of the model parameters included in the checkpoint with the AutoModel class.\n",
    "\n",
    "This is because all transformers models are designed to have a single base class and multiple task-specific prediction classes built on top of it. The AutoModel class is designed to only load the base model parameters such as RobertaModel but not task-specific models such as RobertaForSequenceClassification.\n",
    "\n",
    "In order to identify the exact class name and task of your target checkpoint, you can simply refer to the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abd04eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RobertaForSequenceClassification']\n"
     ]
    }
   ],
   "source": [
    "# Get the model configuration \n",
    "config = model.config\n",
    "\n",
    "# Print model config's architectures attribute to see the model class name\n",
    "print(model.config.architectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799cb52",
   "metadata": {},
   "source": [
    "We found out that the pretrained model we are trying to load is an instance of the RobertaForSequenceClassification class.\n",
    "\n",
    "Important Tip\n",
    "\n",
    "You don't have to download and print out the configuration to identify the model class name. You can simply navigate to the \n",
    "config.json\n",
    " file within the model repository to find it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a97c04",
   "metadata": {},
   "source": [
    "# Task 2: Downstream Tasks and Task-Specific AutoModel Classes\n",
    "* Limitations of AutoModel\n",
    "\n",
    "In the previous task, we learned all about Auto classes and the limitations of the AutoModel class. We also learned that each transformers model consists of a base class and one or more task-specific classes. For example, RoBERTa has the base RobertaModel class and multiple prediction classes such as RobertaForSequenceClassification, RobertaForQuestionAnswering, etc. that leverage the base class.\n",
    "\n",
    "Task-Specific AutoModel Classes\n",
    "\n",
    "Luckily for us, transformers has multiple task-specific AutoModelForTASK classes to load pretrained models. \n",
    "In this task, you will learn how to load a pretrained model with a task-specific Auto model class.\n",
    "\n",
    "* Instructions\n",
    "Load the \n",
    "cardiffnlp/twitter-roberta-base-emoji\n",
    " model we used in the previous task with the correct AutoModelForTASK class.\n",
    "\n",
    "cardiffnlp/twitter-roberta-base-emoji is an instance of the RobertaForSequenceClassification class, take a look at the list of \n",
    "Auto Classes\n",
    "Identify and import the correct Auto class\n",
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "484c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the correct AutoModel class for RobertaForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pretrained \"cardiffnlp/twitter-roberta-base-emoji\" model using the correct auto model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b20fbb",
   "metadata": {},
   "source": [
    "# Task 3: Explicit Model and Preprocesser Class Names\n",
    "We learned how to identify the data preprocessor and model class of a model repository on the Hub in the previous task. Note that you can also do this by navigating to the Files and versions tab on the repository page.\n",
    "\n",
    "Similar to models with multiple task-specific model classes, data preprocessor classes are named based on the model name and the modality.\n",
    "\n",
    "Tokenizer classes for language models\n",
    "FeatureExtractor classes for audio models\n",
    "ImageProcessor classes for computer vision models\n",
    "Processor classes for multi-modal models\n",
    "These data preprocessor classes transform raw input data such as text, image, etc. to the format expected by your target pre-trained model. For an vision model, the preprocessor performs steps such as resizing to a fixed size, normalization, cropping, etc. In this task, you will\n",
    "\n",
    "Use the \"cardiffnlp/twitter-roberta-base-emoji\" model we loaded with Auto classes before Explicitly import a model and its preprocessor class\n",
    "Use explicit class names to load a pretrained model and its preprocessor\n",
    "\n",
    "Instructions\n",
    "Start by explicitly importing the tokenizer and model class of \"cardiffnlp/twitter-roberta-base-emoji\", which we identified in our previous task.\n",
    "\n",
    "Use the from_pretrained method to download and load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faa1d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the explicit tokenizer class of \"cardiffnlp/twitter-roberta-base-emoji\" and the explicit model class of \"cardiffnlp/twitter-roberta-base-emoji\"\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model using the from_pretrained method\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f56b5",
   "metadata": {},
   "source": [
    "# Task 4: Creating an NLP Pipeline - Part 1/2\n",
    "You learned how to navigate the Hugging Face Hub repositories, identify the model and data preprocessor classes of your target repository and how to import and load a pre-trained model and its corresponding preprocessor.\n",
    "\n",
    "You will now put these skills to use and create powerful language translation pipeline with a few lines of code using the \n",
    "google/flan-t5-base\n",
    " model. This multi-lingual translation model allows you to translate between multiple languages. For a full list of languages this model supports, simply refer to its model repository.\n",
    "\n",
    "Text-to-Text Generation\n",
    "\n",
    "In transformers, NLP models with text input and output fall under the AutoModelForSeq2SeqLM class. This class of pretrained models perform tasks such as translation, question answering and text completion.\n",
    "\n",
    "Inputs for T5\n",
    "\n",
    "The tokenizer of T5 preprocesses the raw text input to return its token ids and attention mask. Each token id represents a unique word, tag or punctuation in the model's vocabulary. The attention mask consists of 1s and 0s for each token to decide whether that token should be attended to or ignored by the transformer model.\n",
    "\n",
    "In this task, you will learn\n",
    "\n",
    "What the tokenizer classes do under the hood\n",
    "How to preprocess your raw input text\n",
    "\n",
    "* Instructions\n",
    "\n",
    "Use the AutoTokenizer and AutoModelForSeq2SeqLM classes to load the pre-trained \"google/flan-t5-base\" model and its tokenizer\n",
    "Create a text input that includes\n",
    "A prompt to specify source and target languages for translation\n",
    "Text to be translated\n",
    "Preprocess the text input to retrieve token ids and the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12b5106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AutoModelForSeq2SeqLM from transformers and Load the pre-trained google/flan-t5-base model and tokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6039650",
   "metadata": {},
   "source": [
    "We are ready to perform inference with the pretrained model. We will start by defining an input sentence where we start by which language we would like to translate from and to, and add the sentence to be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9e7b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
      "             1]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Specify source and target languages for translation\n",
    "source_lang = \"English\"\n",
    "target_lang = \"German\"\n",
    "input_text = f\"translate {source_lang} to {target_lang}: How old are you?\"\n",
    "\n",
    "# Use the tokenizer to preprocess input text, with return_tensors=\"pt\" to return PyTorch tensors \n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\") # \"pt\"=pytorch\n",
    "\n",
    "# Print the key and values of the preprocessed inputs dictionary\n",
    "for key, value in inputs.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd125f",
   "metadata": {},
   "source": [
    "# Task 5: Creating an NLP Pipeline - Part 2/2\n",
    "You preprocessed the input text and are ready learn how to perform inference. In this task, you will learn\n",
    "\n",
    "How to use torch.no_grad() to perform inference fast and efficiently, and reduce memory usage\n",
    "How to feed preprocessed inputs to the model\n",
    "How to decode model output to make it human readable\n",
    "Note that transformers and diffusers libraries support batched inputs, meaning you can inputs multiple text inputs or images, etc. as a list. Regardless of the number of inputs, the preprocess and model classes output batched results where each entry corresponds to an input.\n",
    "\n",
    "Instructions\n",
    "Using torch.no_grad()\n",
    "\n",
    "Pass preprocessed inputs to the loaded model to perform inference\n",
    "Save the model output to a variable named outputs\n",
    "Decode the model output tokens to human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "521c64d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (RobertaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'RobertaForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use torch.no_grad() to prevent gradient accumulation - speeds up inference, reduces memory\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Use model.generate method to perform inference\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Use **inputs to pass the unpacked dictionary as multiple arguments to the generate method\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# The output is a list of length batch_size - number of input texts - which is 1 in this case\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Print the length of outputs list\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1323\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   1320\u001b[39m         synced_gpus = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1322\u001b[39m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[32m   1328\u001b[39m     \u001b[38;5;66;03m# three conditions must be met\u001b[39;00m\n\u001b[32m   1329\u001b[39m     \u001b[38;5;66;03m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;66;03m# 2) the generation config must have seen no modification since its creation (the hash is the same);\u001b[39;00m\n\u001b[32m   1331\u001b[39m     \u001b[38;5;66;03m# 3) the user must have set generation parameters in the model config.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1110\u001b[39m, in \u001b[36mGenerationMixin._validate_model_class\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[32m   1109\u001b[39m     exception_message += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[31mTypeError\u001b[39m: The current model class (RobertaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'RobertaForCausalLM'}"
     ]
    }
   ],
   "source": [
    "# Use torch.no_grad() to prevent gradient accumulation - speeds up inference, reduces memory\n",
    "with torch.no_grad():\n",
    "    # Use model.generate method to perform inference\n",
    "    # Use **inputs to pass the unpacked dictionary as multiple arguments to the generate method\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# The output is a list of length batch_size - number of input texts - which is 1 in this case\n",
    "# Print the length of outputs list\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abcde3",
   "metadata": {},
   "source": [
    "The model output is a sequence of token IDs, where each token represents a unique word or punctuation in the vocabular. To decode it to a human readable format, we just need to map each token ID to its corresponding word. All Tokenizer classes of transformers provides a convenient decode method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae3a41c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wie old sind Sie?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the decode method of the tokenizer to convert the output to a human readable format \n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1fcddd",
   "metadata": {},
   "source": [
    "# Task 6: Datasets Library\n",
    "The transformers library is designed to make it easy to load and use pre-trained models, and to preprocess raw input data. However, it is up to you to find or create interesting datasets.\n",
    "\n",
    "We will now take a look at the datasets library and learn how to find, download and create datasets with a few lines of code. This library is part of the Hugging Face ecosystem, and similar to transformers and diffusers, is tightly integrated with the Hugging Face Hub. Each library is stored as a Git repository on the Hub and can be downloaded with a single line of code.\n",
    "\n",
    "In this task, you will learn\n",
    "\n",
    "How to use the load_dataset() function to download and load a dataset from the Hub\n",
    "How to only download a specific data split (training, validation, test)\n",
    "Instructions\n",
    "Import the load_dataset() function from datasets\n",
    "Load the \"adirik/fashion_image_caption-100\" dataset\n",
    "Print the dataset to see what features it contains\n",
    "Print a sample of the dataset\n",
    "Download only the training split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9a07a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 492/492 [00:00<00:00, 492kB/s]\n",
      "Downloading readme: 100%|██████████| 492/492 [00:00<00:00, 492kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 21.77 MiB, generated: 21.78 MiB, post-processed: Unknown size, total: 43.55 MiB) to file://C:/Users/Tiya/.cache/huggingface/datasets/adirik___parquet/adirik--fashion_image_caption-100-522dc40e35adf87a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 22.8M/22.8M [00:04<00:00, 5.69MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n",
      "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.35it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.35it/s]s]\n",
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to file://C:/Users/Tiya/.cache/huggingface/datasets/adirik___parquet/adirik--fashion_image_caption-100-522dc40e35adf87a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     shutil.rmtree(cache_dir)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use the load_dataset function to load the \"adirik/fashion_image_caption-100\" dataset\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# download_mode=\"force_redownload\" will clear the cache and download fresh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madirik/fashion_image_caption-100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforce_redownload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33madirik/fashion_image_caption-100\u001b[39m\u001b[33m\"\u001b[39m, cache_dir=\u001b[33m\"\u001b[39m\u001b[33m./data_cache\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Print the dataset object\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1810\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1807\u001b[39m keep_in_memory = (\n\u001b[32m   1808\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1809\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1810\u001b[39m ds = \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[32m   1812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\builder.py:1128\u001b[39m, in \u001b[36mDatasetBuilder.as_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[39m\n\u001b[32m   1126\u001b[39m is_local = \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m._fs)\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m._fs).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m._output_dir):\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please make sure to call \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbuilder.download_and_prepare(), or use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1134\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "# Import load_dataset from datasets package\n",
    "from datasets import load_dataset\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Clear the specific dataset cache\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets/adirik___fashion_image_caption-100\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "# Use the load_dataset function to load the \"adirik/fashion_image_caption-100\" dataset\n",
    "# download_mode=\"force_redownload\" will clear the cache and download fresh\n",
    "dataset = load_dataset(\"adirik/fashion_image_caption-100\", download_mode=\"force_redownload\")\n",
    "dataset = load_dataset(\"adirik/fashion_image_caption-100\", cache_dir=\"./data_cache\")\n",
    "# Print the dataset object\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e6cb8",
   "metadata": {},
   "source": [
    "Each dataset is a dictionary with train, and optionally val and test splits. Each split contains an iterable subset of the data with one or more keys that represent the features of the dataset defined by the dataset owner. For example, the \"adirik/fashion_image_caption-100\" dataset contains image and text keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c0fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Print the fifth data sample in the dataset - an image and a text caption pair\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m4\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the fifth data sample in the dataset - an image and a text caption pair\n",
    "print(dataset['train'][4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
